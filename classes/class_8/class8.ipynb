{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class : Hidden Markov Models - Forward Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before Class\n",
    "In class today we will be implementing the Forward, Backward, and Forward-Backward algorithms. Forward and Backward are very related to viterbi with minor differences in the calculations. \n",
    "\n",
    "Prior to class, please do the following:\n",
    "1. Review these three algorithms in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "\n",
    "1. Forward algorithm\n",
    "* Backward algorithm\n",
    "* Forward-Backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last class we described Markov chains. Here we expand this idea to the concept of a hidden state variable along with observed emissions from the model. We will be using the example of CpG islands from the lecture slides. I have provided the class structure of a simple HMM below. All parameters to this model must be provided as inputs, so essentially this is a class containing the parameters described below:\n",
    "\n",
    "We define a categorical Hidden Markov Model as $M = (\\Sigma, Q, \\Theta)$ with the following parameters:\n",
    "\n",
    "* $\\Sigma$ : Finite alphabet of symbols (eg. A, C, G, T)\n",
    "\n",
    "* $Q$ : Finite discrete hidden states\n",
    "\n",
    "* $\\Theta$: set of probabilities containing: $A$ as transition probabilites $a_{kl}$ for all $k,l \\in Q$ and $E$ as emission probabilities $e_k(\\sigma)$ for all $k \\in Q$ and $\\sigma \\in \\Sigma$ and $B$ as starting probabilities $b_k$ for all $k \\in Q$.\n",
    "\n",
    "We also define a number of $T$ emissions as $y_t = 1 \\dots T$ that are drawn from $\\Sigma$ and hidden states as $\\pi_t = 1 \\dots T$ that are drawn from $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Forward algorithm\n",
    "\n",
    "The Forward algorithm can be used to estimate the probability of the sequence given our HMM. In general, this is the same as Viterbi except that we sum probablilities instead of taking the max, and we do not need traceback. This is described as bellow:\n",
    "\n",
    "To estimate $P(x)$, the probability of sequence $x$ given our HMM, calculate forward algorithm values as $f()$ using:\n",
    "> Initialization ($i = 0$): $f_{k}(0) = e_{k}(\\sigma_{0})b_{k}$.<br>\n",
    "> Recursion ($i = 1 \\dots T$): $f_{l}(i) = e_{l}(\\sigma_{i})\\sum_{k}(f_{k}(i-1)a_{kl})$<br>\n",
    "> Termination: $P(x) = \\sum_{k}f_{k}(T)$\n",
    "\n",
    "## Backward algorithm\n",
    "The backward algorithm is essentially the reverse of the forward algorithm. To estimate $P(x)$, the probability of sequence $x$ given our HMM, calculate the backward algorithm values as $r()$ using:\n",
    ">Initialization ($i = T$): $r_{k}(T) = 1$.<br>\n",
    ">Recursion ($i = T-1 \\dots 1$): $r_{k}(i) = \\sum_{l}r_{l}(i+1)a_{kl}e_{l}(\\sigma_{i+1})$<br>\n",
    ">Termination: $P(x) = \\sum_{l}r_{k}(1)e_l(\\sigma_{1})b_{l}$\n",
    "\n",
    "## Forward-Backward algorithm\n",
    "The Forward-Backward algorithm is an extension of both the Forward and Backward algorithms and can be used to estimate the marginal posterior probability of our sequence $x$ being in a state at a specific time. We can calculate this value at every position $i$ and state $k$ as:\n",
    ">$P(\\pi_{i} = k | x) = f_{k}(i)b_{k}(i) / P(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \"\"\"Main class for HMM objects\n",
    "    \n",
    "    Class for holding HMM parameters and to allow for implementation of\n",
    "    functions associated with HMMs\n",
    "    \n",
    "    Private Attributes:\n",
    "        _alphabet (set): The alphabet of emissions\n",
    "        _hidden_states (set): Hidden states in the model\n",
    "        _transitions (dict(dict)): A dictionary of transition probabilities\n",
    "        _emissions (dict(dict)): A dictionary of emission probabilities\n",
    "        _initial (dict): A dictionary of initial state probabilities\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    __all__ = ['viterbi', 'forward', 'backward', 'forward_backward']\n",
    "\n",
    "    def __init__(self, alphabet, hidden_states, A=None, E=None, B=None):\n",
    "        self._alphabet = set(alphabet)\n",
    "        self._hidden_states = set(hidden_states)\n",
    "        self._transitions = A\n",
    "        self._emissions = E\n",
    "        self._initial = B\n",
    "        if(self._transitions == None):\n",
    "            self._initialize_random(self._alphabet, self._hidden_states)\n",
    "            \n",
    "    def __str__(self):\n",
    "        out_text = [f'Alphabet: {self._alphabet}',\n",
    "                    f'Hidden States: {self._hidden_states}',\n",
    "                    f'Initial Probabilities: {json.dumps(self._initial, sort_keys = True, indent=4)}',\n",
    "                    f'Transition Probabilities: {json.dumps(self._transitions, sort_keys = True, indent=4)}',\n",
    "                    f'Emission Probabilities: {json.dumps(self._emissions, sort_keys = True, indent=4)}']\n",
    "        return '\\n'.join(out_text)\n",
    "    \n",
    "    @classmethod\n",
    "    def __dir__(cls):\n",
    "        return cls.__all__\n",
    "        \n",
    "    def _emit(self, cur_state, symbol):\n",
    "        return self._emissions[cur_state][symbol]\n",
    "    \n",
    "    def _transition(self, cur_state, next_state):\n",
    "        return self._transitions[cur_state][next_state]\n",
    "    \n",
    "    def _init(self, cur_state):\n",
    "        return self._initial[cur_state]\n",
    "\n",
    "    def _states(self):\n",
    "        for k in self._hidden_states:\n",
    "            yield k\n",
    "    \n",
    "    def _get_alphabet(self):\n",
    "        for sigma in self._alphabet:\n",
    "            yield sigma\n",
    "            \n",
    "    def _initialize_random(self, alphabet, states):\n",
    "        self._alphabet = set(alphabet)\n",
    "        self._hidden_states = set(hidden_states)\n",
    "\n",
    "        #Initialize empty matrices A and E with pseudocounts\n",
    "        A = {}\n",
    "        E = {}\n",
    "        I = {}\n",
    "        I_rand = np.random.dirichlet(np.ones(len(self._hidden_states)))\n",
    "        for i, state in enumerate(self._states()):\n",
    "            E[state] = {}\n",
    "            A[state] = {}\n",
    "            I[state] = I_rand[i]\n",
    "            E_rand = np.random.dirichlet(np.ones(len(self._alphabet)))\n",
    "            A_rand = np.random.dirichlet(np.ones(len(self._hidden_states)))\n",
    "            for j, sigma in enumerate(self._get_alphabet()):\n",
    "                E[state][sigma] = E_rand[j]\n",
    "            for j, next_state in enumerate(self._states()):\n",
    "                A[state][next_state] = A_rand[j]\n",
    "                \n",
    "        self._transitions = A\n",
    "        self._emissions = E\n",
    "        self._initial = I\n",
    "        return\n",
    "        \n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\" The viterbi algorithm for decoding a string using a HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (list): optimal path through HMM given the model parameters\n",
    "                           using the Viterbi algorithm\n",
    "        \n",
    "        Pseudocode for Viterbi:\n",
    "            Initialization (𝑖=0): 𝑣𝑘(𝑖)=𝑒𝑘(𝜎)𝑏𝑘.\n",
    "            Recursion (𝑖=1…𝑇): 𝑣𝑙(𝑖)=𝑒𝑙(𝑥𝑖) max𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙); \n",
    "                                ptr𝑖(𝑙)= argmax𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙).\n",
    "            Termination: 𝑃(𝑥,𝜋∗)= max𝑘(𝑣𝑘(𝑙)𝑎𝑘0); \n",
    "                             𝜋∗𝑙= argmax𝑘(𝑣𝑘(𝑙)𝑎𝑘0).\n",
    "            Traceback: (𝑖=𝑇…1): 𝜋∗𝑖−1= ptr𝑖(𝜋∗𝑖).\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialization (𝑖=0): 𝑣𝑘(𝑖)=𝑒𝑘(𝜎)𝑏𝑘.\n",
    "        # Initialize trellis and traceback matrices\n",
    "        # trellis will hold the vi data as defined by Durbin et al.\n",
    "        # and trackback will hold back pointers\n",
    "        trellis = {} # This only needs to keep the previous column probabilities\n",
    "        traceback = [] # This will need to hold all of the traceback data so will be an array of dicts()\n",
    "        for state in self._states():\n",
    "            trellis[state] = np.log10(self._init(state)) + np.log10(self._emit(state, sequence[0])) # b * e(0) for all k\n",
    "            \n",
    "        # Next we do the recursion step:\n",
    "        # Recursion (𝑖=1…𝑇): 𝑣𝑙(𝑖)=𝑒𝑙(𝑥𝑖) max𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙); \n",
    "        #                 ptr𝑖(𝑙)= argmax𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙).\n",
    "        for t in range(1, len(sequence)):  # For each position in the sequence\n",
    "            trellis_next = {}\n",
    "            traceback_next = {}\n",
    "\n",
    "            for next_state in self._states():    # Calculate maxk and argmaxk\n",
    "                k={}\n",
    "                for cur_state in self._states():\n",
    "                    k[cur_state] = trellis[cur_state] + np.log10(self._transition(cur_state, next_state)) # k(t-1) * a\n",
    "                argmaxk = max(k, key=k.get)\n",
    "                trellis_next[next_state] =  np.log10(self._emit(next_state, sequence[t])) + k[argmaxk] # k * e(t)\n",
    "                traceback_next[next_state] = argmaxk\n",
    "                \n",
    "            #Overwrite trellis \n",
    "            trellis = trellis_next\n",
    "            #Keep trackback pointer matrix\n",
    "            traceback.append(traceback_next)\n",
    "            \n",
    "        # Termination: 𝑃(𝑥,𝜋∗)= max𝑘(𝑣𝑘(𝑙)𝑎𝑘0); \n",
    "        #                  𝜋∗𝑙= argmax𝑘(𝑣𝑘(𝑙)𝑎𝑘0).\n",
    "        max_final_state = max(trellis, key=trellis.get)\n",
    "        max_final_prob = trellis[max_final_state]\n",
    "                \n",
    "        # Traceback: (𝑖=𝑇…1): 𝜋∗𝑖−1= ptr𝑖(𝜋∗𝑖).\n",
    "        result = [max_final_state]\n",
    "        for t in reversed(range(len(sequence)-1)):\n",
    "            result.append(traceback[t][max_final_state])\n",
    "            max_final_state = traceback[t][max_final_state]\n",
    "\n",
    "        return result[::-1]\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\" The forward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (float, list of dicts): P(x) and the f matrix as a list\n",
    "        \n",
    "        Pseudocode for Forward:\n",
    "            Initialization (𝑖=0): 𝑓𝑘(0)=𝑒𝑘(𝜎0)𝑏𝑘.\n",
    "            Recursion (𝑖=1…𝑇): 𝑓𝑙(𝑖)=𝑒𝑙(𝜎𝑖)∑𝑘(𝑓𝑘(𝑖−1)𝑎𝑘𝑙)\n",
    "            Termination: 𝑃(𝑥)=∑𝑘𝑓𝑘(𝑇)\n",
    "        \"\"\"\n",
    "        \n",
    "        #init\n",
    "        states = list( self._states() ) #define deterministic ordering of states (get around `set`)\n",
    "        trace = [ {s:self._init(s)*self._emit(s,sequence[0]) for s in states} ] #calc init probs\n",
    "        \n",
    "        #recurse\n",
    "        for b in sequence[1:]: #loop over remainder of sequence\n",
    "            trace.append( dict() ) #add a new dictionary\n",
    "            for s in states:\n",
    "                pos = ( self._emit(s,b)*self._transition(ls,s)*trace[-2][ls] for ls in states ) #calc lambda probs\n",
    "                trace[-1][s] = sum( pos ) #sum lambda probs\n",
    "                \n",
    "        #term\n",
    "        total = sum( trace[-1].values() )\n",
    "                \n",
    "        return total, trace\n",
    "\n",
    "    def backward(self, sequence):\n",
    "        \"\"\" The backward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (float, list of dicts): P(x) and the r matrix as a list\n",
    "        \n",
    "        Pseudocode for Backward:\n",
    "            Initialization (𝑖=T): 𝑟𝑘(𝑇)=1.\n",
    "            Recursion (𝑖=𝑇−1…1): 𝑟𝑘(𝑖)=∑𝑙𝑟𝑙(𝑖+1)𝑎𝑘𝑙𝑒𝑙(𝜎𝑖+1)\n",
    "            Termination: 𝑃(𝑥)=∑𝑙𝑟𝑘(1)𝑒𝑙(𝜎1)𝑏𝑙\n",
    "        \"\"\"\n",
    "        \n",
    "        #init\n",
    "        states = list( self._states() ) #define deterministic ordering of states (get around `set`)\n",
    "        trace = [ {s:1.0 for s in states} ] #calc init probs\n",
    "        \n",
    "        #recurse\n",
    "        for b in sequence[::-1]: #loop over remainder of sequence\n",
    "            trace.append( dict() )\n",
    "            for s in states:\n",
    "                pos = ( self._emit(ls,b)*self._transition(s,ls)*trace[-2][ls] for ls in states ) #calc lambda probs\n",
    "                trace[-1][s] = sum( pos ) #sum lambda probs\n",
    "        \n",
    "        #term\n",
    "        total = sum( self._init(s)*trace[-1][s] for s in states ) \n",
    "                \n",
    "        return total, trace[::-1][1:]\n",
    "    \n",
    "    def forward_backward(self, sequence):\n",
    "        \"\"\" The forward-backward algorithm for calculating marginal posteriors given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            posterior (list of dicts): all posteriors as a list\n",
    "        \n",
    "        Pseudocode for Forward-Backward:\n",
    "            Calculate f[] as forward algorithm\n",
    "            Calculate r[] as backward algorithm\n",
    "            for all i in sequence\n",
    "                for all states\n",
    "                    posterior[i][state] = f[i][state] * r[i][state] / Px\n",
    "        \"\"\"\n",
    "                \n",
    "        p, fi = self.forward( sequence )\n",
    "        p, ri = self.backward( sequence )\n",
    "                \n",
    "        return [ {s:f[s]*r[s]/p for s in self._states()} for f,r in zip(fi,ri) ]                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet: {'T', 'G', 'C', 'A'}\n",
      "Hidden States: {'G', 'I'}\n",
      "Initial Probabilities: {\n",
      "    \"G\": 0.9,\n",
      "    \"I\": 0.1\n",
      "}\n",
      "Transition Probabilities: {\n",
      "    \"G\": {\n",
      "        \"G\": 0.9,\n",
      "        \"I\": 0.1\n",
      "    },\n",
      "    \"I\": {\n",
      "        \"G\": 0.4,\n",
      "        \"I\": 0.6\n",
      "    }\n",
      "}\n",
      "Emission Probabilities: {\n",
      "    \"G\": {\n",
      "        \"A\": 0.4,\n",
      "        \"C\": 0.1,\n",
      "        \"G\": 0.1,\n",
      "        \"T\": 0.4\n",
      "    },\n",
      "    \"I\": {\n",
      "        \"A\": 0.1,\n",
      "        \"C\": 0.4,\n",
      "        \"G\": 0.4,\n",
      "        \"T\": 0.1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# This section of code will initialize your HMM with parameters as defined in the lecture slides\n",
    "# for the identification of CpG Islands.\n",
    "# All of this should be able to run whether or not you implement the functions!\n",
    "\n",
    "hidden_states = ('I', 'G') # CpG Island or Genome\n",
    "alphabet = ('A', 'C', 'G', 'T') # DNA Alphabet\n",
    "\n",
    "# These are the initial probabilities as defined in the lecture slides\n",
    "initial_probabilities = {\n",
    "    'I' : 0.1,\n",
    "    'G' : 0.9\n",
    "}\n",
    "\n",
    "# These are the probabilities of transitioning from outer state to inner state\n",
    "#  as defined in the lecture slides\n",
    "transition_probabilities = {\n",
    "    'I': { 'I' : 0.6, 'G' : 0.4 },\n",
    "    'G': { 'I' : 0.1, 'G' : 0.9 }\n",
    "}\n",
    "\n",
    "# These are the probabilites of each state emmitting each alphabet character\n",
    "emission_probabilities = {\n",
    "    'I': { 'A' : 0.1, 'C' : 0.4, 'G' : 0.4, 'T' : 0.1 },\n",
    "    'G': { 'A' : 0.4, 'C' : 0.1, 'G' : 0.1, 'T' : 0.4 }\n",
    "}\n",
    "\n",
    "# Build the model\n",
    "model = HMM(alphabet, hidden_states, transition_probabilities, emission_probabilities, initial_probabilities)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:\n",
      "5.638948422400004e-06 [{'G': 0.36000000000000004, 'I': 0.010000000000000002}, {'G': 0.0328, 'I': 0.016800000000000006}, {'G': 0.003624000000000001, 'I': 0.0053440000000000015}, {'G': 0.0005399200000000003, 'I': 0.0014275200000000003}, {'G': 0.00010569360000000005, 'I': 0.0003642016000000001}, {'G': 9.632195200000003e-05, 'I': 2.2909032000000005e-05}, {'G': 3.834134784000002e-05, 'I': 2.3377614400000006e-06}, {'G': 3.5442317632000023e-06, 'I': 2.0947166592000014e-06}]\n",
      "\n",
      "Backward:\n",
      "5.626100089600004e-06 [{'G': 1.416161272000001e-05, 'I': 5.407678432000003e-05}, {'G': 6.178578400000004e-05, 'I': 0.0002150223040000001}, {'G': 0.0003113848000000002, 'I': 0.0008440288000000004}, {'G': 0.0020485600000000014, 'I': 0.0031753600000000016}, {'G': 0.01823200000000001, 'I': 0.010192000000000005}, {'G': 0.04960000000000001, 'I': 0.03760000000000001}, {'G': 0.13, 'I': 0.28}, {'G': 1.0, 'I': 1.0}]\n",
      "\n",
      "Posterior:\n",
      "[{'G': 0.9061659938514295, 'I': 0.09611770757502593}, {'G': 0.36020932491872604, 'I': 0.6420743765077294}, {'G': 0.20057562027486617, 'I': 0.8017080811515892}, {'G': 0.19659417670947232, 'I': 0.8056895247169831}, {'G': 0.34251180827055017, 'I': 0.6597718931559052}, {'G': 0.849179492563857, 'I': 0.153104208862598}, {'G': 0.8859378858925302, 'I': 0.11634581553392485}, {'G': 0.629962444100774, 'I': 0.3723212573256812}]\n"
     ]
    }
   ],
   "source": [
    "# Exact example from slides\n",
    "sequence = \"ACGCGATC\"\n",
    "\n",
    "print (\"Forward:\")\n",
    "f_Px, f_matrix = model.forward(list(sequence))\n",
    "print (f_Px, f_matrix)\n",
    "\n",
    "print (\"\\nBackward:\")\n",
    "r_Px, r_matrix = model.backward(list(sequence))\n",
    "print (r_Px, r_matrix)\n",
    "\n",
    "print (\"\\nPosterior:\")\n",
    "posterior = model.forward_backward(list(sequence))\n",
    "print (posterior)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "Forward:\n",
    "5.638948422400005e-06 [{'G': 0.36000000000000004, 'I': 0.010000000000000002}, {'G': 0.03280000000000001, 'I': 0.016800000000000002}, {'G': 0.003624000000000001, 'I': 0.0053440000000000015}, {'G': 0.0005399200000000002, 'I': 0.0014275200000000005}, {'G': 0.00010569360000000006, 'I': 0.00036420160000000013}, {'G': 9.632195200000006e-05, 'I': 2.290903200000001e-05}, {'G': 3.8341347840000026e-05, 'I': 2.3377614400000015e-06}, {'G': 3.544231763200003e-06, 'I': 2.094716659200002e-06}]\n",
    "\n",
    "Backward:\n",
    "5.638948422400003e-06 [{'G': 1.4161612720000007e-05, 'I': 5.407678432000002e-05}, {'G': 6.178578400000003e-05, 'I': 0.0002150223040000001}, {'G': 0.0003113848000000001, 'I': 0.0008440288000000003}, {'G': 0.0020485600000000005, 'I': 0.003175360000000001}, {'G': 0.018232000000000005, 'I': 0.010192000000000003}, {'G': 0.049600000000000005, 'I': 0.03760000000000001}, {'G': 0.13, 'I': 0.28}, {'G': 1, 'I': 1}]\n",
    "\n",
    "Posterior:\n",
    "[{'G': 0.9041012964311091, 'I': 0.09589870356889041}, {'G': 0.3593885886860918, 'I': 0.640611411313908}, {'G': 0.20011860912175444, 'I': 0.7998813908782453}, {'G': 0.19614623726763022, 'I': 0.8038537627323695}, {'G': 0.3417313957945096, 'I': 0.6582686042054902}, {'G': 0.8472446387737329, 'I': 0.1527553612262669}, {'G': 0.8839192781760881, 'I': 0.11608072182391166}, {'G': 0.6285270759209276, 'I': 0.37147292407907234}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
