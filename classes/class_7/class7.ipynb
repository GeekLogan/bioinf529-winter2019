{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class : Hidden Markov Models - Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before Class\n",
    "In class today we will be implementing the Viterbi algorithm to identify the most likely path through states given model parameters.\n",
    "Prior to class, please do the following:\n",
    "1. Review slides on Hidden Markov models in detail\n",
    "* Focus on how to conceptually translate the algorithm to code\n",
    "* Understand what argmax versus max means\n",
    "* How does one implement a max function? argmax?\n",
    "* Take a look at what arithmetic underflow is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "\n",
    "1. Conceptually understand Hidden Markov Models\n",
    "* Implement a basic HMM\n",
    "* Implement the Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last class we described Markov chains. Here we expand this idea to the concept of a hidden state variable along with observed emissions from the model. We will be using the example of CpG islands from the lecture slides. I have provided the class structure of a simple HMM below. All parameters to this model must be provided as inputs, so essentially this is a class containing the parameters described below:\n",
    "\n",
    "We define a categorical Hidden Markov Model as $M = (\\Sigma, Q, \\Theta)$ with the following parameters:\n",
    "\n",
    "$\\Sigma$ : Finite alphabet of symbols (eg. A, C, G, T)\n",
    "\n",
    "$Q$ : Finite discrete hidden states\n",
    "\n",
    "$\\Theta$: set of probabilities containing: $A$ as transition probabilites $a_{kl}$ for all $k,l \\in Q$ and $E$ as emission probabilities $e_k(\\sigma)$ for all $k \\in Q$ and $\\sigma \\in \\Sigma$ and $B$ as starting probabilities $b_k$ for all $k \\in Q$.\n",
    "\n",
    "We also define a number of $T$ emissions as $y_t = 1 \\dots T$ that are drawn from $\\Sigma$ and hidden states as $\\pi_t = 1 \\dots T$ that are drawn from $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal today will be to estimate $\\pi^*$, the most probable path through the hidden states $Q$ when a HMM $M$ is provided.\n",
    "\n",
    "We will be following the definition described in the slides as described below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Viterbi algorithm\n",
    "\n",
    "To estimate $\\pi^*$, the most probable path through the hidden states, we will use the Viterbi algorithm, which is a dynamic programming exercise.\n",
    "\n",
    "Initialization ($i = 0$): $v_{k}(i) = e_{k}(\\sigma)b_{k}$.\n",
    "\n",
    "Recursion ($i = 1 \\dots T$): \n",
    "$v_{l}(i) = e_{l}(x_{i})$ max$_{k}(v_{k}(i-1)a_{kl})$;  ptr$_{i}(l) = $ argmax$_{k}(v_{k}(i-1)a_{kl})$.\n",
    "\n",
    "Termination: $P(x, \\pi^{*}) =$ max$_{k}(v_{k}(l)a_{k0})$; $\\pi^{*}_{l} = $ argmax$_{k}(v_{k}(l)a_{k0})$.\n",
    "\n",
    "Traceback: ($i = T\\dots1$): $\\pi^{*}_{i-1} = $ ptr$_{i}(\\pi^{*}_{i})$.\n",
    "\n",
    "A few implementation notes:\n",
    "1. Break the code up into each of the above phases of the algorithm!\n",
    "2. You will probably want to move all of your probabilities into log space so that you don't get underflow errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \"\"\"Main class for HMM objects\n",
    "    \n",
    "    Class for holding HMM parameters and to allow for implementation of\n",
    "    functions associated with HMMs\n",
    "    \n",
    "    Private Attributes:\n",
    "        _alphabet (set): The alphabet of emissions\n",
    "        _hidden_states (set): Hidden states in the model\n",
    "        _transitions (dict(dict)): A dictionary of transition probabilities\n",
    "        _emissions (dict(dict)): A dictionary of emission probabilities\n",
    "        _initial (dict): A dictionary of initial state probabilities\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, hidden_states, A=None, E=None, B=None):\n",
    "        self._alphabet = set(alphabet)\n",
    "        self._hidden_states = set(hidden_states)\n",
    "        self._transitions = A\n",
    "        self._emissions = E\n",
    "        self._initial = B\n",
    "        \n",
    "    def _emit(self, cur_state, symbol):\n",
    "        return self._emissions[cur_state][symbol]\n",
    "    \n",
    "    def _transition(self, cur_state, next_state):\n",
    "        return self._transitions[cur_state][next_state]\n",
    "    \n",
    "    def _init(self, cur_state):\n",
    "        return self._initial[cur_state]\n",
    "\n",
    "    def _states(self):\n",
    "        for k in self._hidden_states:\n",
    "            yield k\n",
    "        \n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\" The viterbi algorithm for decoding a string using a HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (list): optimal path through HMM given the model parameters\n",
    "                           using the Viterbi algorithm\n",
    "        \n",
    "        Pseudocode for Viterbi:\n",
    "            Initialization (𝑖=0): 𝑣𝑘(𝑖)=𝑒𝑘(𝜎)𝑏𝑘.\n",
    "            Recursion (𝑖=1…𝑇): 𝑣𝑙(𝑖)=𝑒𝑙(𝑥𝑖) max𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙); \n",
    "                                ptr𝑖(𝑙)= argmax𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙).\n",
    "            Termination: 𝑃(𝑥,𝜋∗)= max𝑘(𝑣𝑘(𝑙)𝑎𝑘0); \n",
    "                             𝜋∗𝑙= argmax𝑘(𝑣𝑘(𝑙)𝑎𝑘0).\n",
    "            Traceback: (𝑖=𝑇…1): 𝜋∗𝑖−1= ptr𝑖(𝜋∗𝑖).\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialization (𝑖=0): 𝑣𝑘(𝑖)=𝑒𝑘(𝜎)𝑏𝑘.\n",
    "        \n",
    "        # Recursion (𝑖=1…𝑇): 𝑣𝑙(𝑖)=𝑒𝑙(𝑥𝑖) max𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙); \n",
    "        #                 ptr𝑖(𝑙)= argmax𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙).\n",
    "            \n",
    "        # Termination: 𝑃(𝑥,𝜋∗)= max𝑘(𝑣𝑘(𝑙)𝑎𝑘0); \n",
    "        #                  𝜋∗𝑙= argmax𝑘(𝑣𝑘(𝑙)𝑎𝑘0).\n",
    "\n",
    "        # Traceback: (𝑖=𝑇…1): 𝜋∗𝑖−1= ptr𝑖(𝜋∗𝑖).\n",
    "        \n",
    "        #init\n",
    "        states = list( self._states() ) #define deterministic ordering of states (get around `set`)\n",
    "        trace = [ {s:(np.log(self._init(s)*self._emit(s,sequence[0])), None) for s in states} ] #calc init probs\n",
    "        \n",
    "        #recurse\n",
    "        for b in sequence[1:]: #loop over remainder of sequence\n",
    "            trace.append( dict() ) #add a new dictionary\n",
    "            for s in states:\n",
    "                pos = [ np.log( self._emit(s,b) * self._transition(ls,s) ) + trace[-2][ls][0] for ls in states ] #calc lambda probs\n",
    "                trace[-1][s] = max( zip(pos,states) ) #optimize lambda probs\n",
    "                \n",
    "        #term\n",
    "        ptr = max( ( trace[-1][s][0], s, trace[-1][s][1] ) for s in states ) #find the max endpt\n",
    "        toreturn, ptr = [ ptr[1] ], ptr[2] #define iterators for traceback\n",
    "        \n",
    "        #traceback\n",
    "        for level in trace[-1::-1]: #go back up the stack\n",
    "            toreturn.insert( 0, ptr ) #save old pointer to front of stack\n",
    "            ptr = level[ptr][1] #define new pointer\n",
    "            \n",
    "        return toreturn\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code will initialize your HMM with parameters as defined in the lecture slides\n",
    "# for the identification of CpG Islands.\n",
    "# All of this should be able to run whether or not you implement the Viterbi function!\n",
    "\n",
    "hidden_states = ('I', 'G') # CpG Island or Genome\n",
    "alphabet = ('A', 'C', 'G', 'T') # DNA Alphabet\n",
    "\n",
    "# These are the initial probabilities as defined in the lecture slides\n",
    "initial_probabilities = {\n",
    "    'I' : 0.1,\n",
    "    'G' : 0.9\n",
    "}\n",
    "\n",
    "# These are the probabilities of transitioning from outer state to inner state\n",
    "#  as defined in the lecture slides\n",
    "transition_probabilities = {\n",
    "    'I': { 'I' : 0.6, 'G' : 0.4 },\n",
    "    'G': { 'I' : 0.1, 'G' : 0.9 }\n",
    "}\n",
    "\n",
    "# These are the probabilites of each state emmitting each alphabet character\n",
    "emission_probabilities = {\n",
    "    'I': { 'A' : 0.1, 'C' : 0.4, 'G' : 0.4, 'T' : 0.1 },\n",
    "    'G': { 'A' : 0.4, 'C' : 0.1, 'G' : 0.1, 'T' : 0.4 }\n",
    "}\n",
    "\n",
    "# Build the model\n",
    "model = HMM(alphabet, hidden_states, transition_probabilities, emission_probabilities, initial_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACGCGATC\n",
      "GIIIIGGGG\n",
      "ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\n",
      "GIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIGGGGGG\n"
     ]
    }
   ],
   "source": [
    "# Exact example from slides\n",
    "sequence = \"ACGCGATC\"\n",
    "print(sequence)\n",
    "print (''.join(model.viterbi(list(sequence))))\n",
    "\n",
    "# A slightly more complex example\n",
    "sequence = \"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\"\n",
    "print(sequence)\n",
    "print (''.join(model.viterbi(sequence)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "ACGCGATC\n",
    "GIIIIGGG\n",
    "ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\n",
    "GIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIGGGGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = ('Ai', 'Ci', 'Gi', 'Ti', 'Ag', 'Cg', 'Gg', 'Tg')\n",
    "alphabet = ('A', 'C', 'G', 'T')\n",
    "\n",
    "initial_probabilities = {\n",
    "    'Ai' : 0.125,\n",
    "    'Ci' : 0.125,\n",
    "    'Gi' : 0.125,\n",
    "    'Ti' : 0.125,\n",
    "    'Ag' : 0.125,\n",
    "    'Cg' : 0.125,\n",
    "    'Gg' : 0.125,\n",
    "    'Tg' : 0.125\n",
    "}\n",
    "\n",
    "transition_probabilities = {\n",
    "    'Ai': { 'Ai' : 0.2, 'Ci' : 0.36, 'Gi' : 0.2, 'Ti' : 0.2, 'Ag' : 0.01, 'Cg' : 0.01, 'Gg' : 0.01, 'Tg' : 0.01 },\n",
    "    'Ci': { 'Ai' : 0.1, 'Ci' : 0.1, 'Gi' : 0.66, 'Ti' : 0.1, 'Ag' : 0.01, 'Cg' : 0.01, 'Gg' : 0.01, 'Tg' : 0.01 },\n",
    "    'Gi': { 'Ai' : 0.1, 'Ci' : 0.39, 'Gi' : 0.1, 'Ti' : 0.1, 'Ag' : 0.1, 'Cg' : 0.01, 'Gg' : 0.1, 'Tg' : 0.1 },\n",
    "    'Ti': { 'Ai' : 0.2, 'Ci' : 0.36, 'Gi' : 0.2, 'Ti' : 0.2, 'Ag' : 0.01, 'Cg' : 0.01, 'Gg' : 0.01, 'Tg' : 0.01 },\n",
    "    'Ag': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 },\n",
    "    'Cg': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 },\n",
    "    'Gg': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 },\n",
    "    'Tg': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 }\n",
    "}\n",
    "\n",
    "emission_probabilities = {\n",
    "    'Ai': { 'A' : 1, 'C' : 0.001, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Ci': { 'A' : 0.001, 'C' : 1, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Gi': { 'A' : 0.001, 'C' : 0.001, 'G' : 1, 'T' : 0.001 },\n",
    "    'Ti': { 'A' : 0.001, 'C' : 0.001, 'G' : 0.001, 'T' : 1 },\n",
    "    'Ag': { 'A' : 1, 'C' : 0.001, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Cg': { 'A' : 0.001, 'C' : 1, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Gg': { 'A' : 0.001, 'C' : 0.001, 'G' : 1, 'T' : 0.001 },\n",
    "    'Tg': { 'A' : 0.001, 'C' :0.0010, 'G' : 0.001, 'T' : 1 }\n",
    "}\n",
    "\n",
    "model = HMM(alphabet, hidden_states, transition_probabilities, emission_probabilities, initial_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\n",
      "IIIIIggggggggggggggggggggggggggIIIIIIIIIIIIIIggggggggggggggggggggggggggggggggggggggggggggg\n"
     ]
    }
   ],
   "source": [
    "sequence = \"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\"\n",
    "\n",
    "print(sequence)\n",
    "\n",
    "result = ''.join(model.viterbi(sequence))\n",
    "result = result.replace(\"A\", \"\")\n",
    "result = result.replace(\"C\", \"\")\n",
    "result = result.replace(\"G\", \"\")\n",
    "result = result.replace(\"T\", \"\")\n",
    "result = result.replace(\"i\", \"I\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\n",
    "IIIIIggggggggggggggggggggggggggIIIIIIIIIIIIIIgggggggggggggggggggggggggggggggggggggggggggg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
