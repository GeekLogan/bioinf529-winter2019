{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOINF529 Exam #1 - Winter 2019\n",
    "This exam is worth 15% of your final grade.\n",
    "\n",
    "The exam is due 1 week after assigned as enforced by Canvas.<br>\n",
    "By turning in this exam you are indicating your acceptance of this statement: “I pledge that this examination is solely my own work.” <br>\n",
    "You may not consult or collaborate with others. All answers must be your own. <br>\n",
    "You are allowed to ask questions at office hours but the answers given will be high-level/conceptual in nature. <br>\n",
    "\n",
    "Please rename this notebook to **exam1_uniqname.ipynb** for submission!<br>\n",
    "\n",
    "Topic areas: motif finding, MCMC, HMMs<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nth order Markov chain\n",
    "We previously discussed and implemented a 1st order Markov chain for processing words and generating random sentences from Dr. Seuss. In this model, each event or word is output from only the previous state with no memory of any prior states. While this is useful in some cases, typical biological applications of Markov chains require higher-order models to accurately capture what we know about a system. For instance, in attempting to identify coding regions of a genome, we know that open reading frames (ORFs) contain codon triplets, and so a third or sixth order Markov chain would better describe these regions. In this exam, you will implement a generalized form of our previous Markov Chain to allow for Nth order chains.\n",
    "\n",
    "As in class, please provide at least two functions (additional helper functions are left to your discretion) that will provide the following:\n",
    "- build_markov_model(markov_model, text, order=N): as in class, takes as input an existing markov model and updates it with 'text'. The updates now will be as a Nth order rather than a 1st order model.\n",
    "- generate_random_text(markov_model): as in class this function returns a randomly generated series of states from the Markov movel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please implement all of your functions for question 1 below. The two functions described below are required.\n",
    "\n",
    "def build_markov_model(markov_model, text, order=1):\n",
    "    '''\n",
    "    Function to build or add to a Nth order Markov model given a string of text\n",
    "\n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "            or None if a new model is being built\n",
    "        new_text (str): a string to build or add to the moarkov_model\n",
    "        order (int): the number of previous states to consider for the model\n",
    "        \n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated/new markov_model\n",
    "    '''\n",
    "    \n",
    "def generate_random_text(markov_model):\n",
    "    '''\n",
    "    Function to generate random text given a markov model\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example testing of the above implementation:\n",
    "text = \"One fish two fish red fish blue fish\"\n",
    "markov_model = build_markov_model(markov_model, text, 1)\n",
    "print (markov_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baum-Welch\n",
    "In class we have now implemented aspects of evaluating a hidden Markov model through Viterbi and the Forward, Backward, and Forward-Backward algorithms. Each of these algorithms requires prior knowledge of dataset labels in order to generate the state inititaion, transition, and emission probabilities. You will now implement the Baum-Welch algorithm which will learn these probabilities for your model in an unsupervised manner. For this assignement, you may use any of the previously provided class implementations as part of your submission.\n",
    "\n",
    "Initialization: <br>\n",
    "Set arbitrary parameters for $\\Theta$ in $A$ transition probabilities, $E$ emission probabilities, and $B$ starting probabilities. Alternatively, set arbitrary state labels $\\pi$ and use these to estimate parameters.\n",
    "\n",
    "Expectation:<br>\n",
    "For each input sequence $x$ where sequence index $j = 1 \\dots n $:<br>\n",
    "Calculate $f_{k}(i)$ matrix for sequence $x$ using the Forward algorithm.<br>\n",
    "Calculate $r_{k}(i)$ matrix for sequence $x$ using the Backward algorithm.<br>\n",
    "Update transition matrix $A_{kl}$ by summing over all positions ($i=1\\dots T-1$):<br>\n",
    "$A_{kl} = \\sum_{j}1/P(x^{j}) \\sum_{i}f_{k}^{j}(i)a_{kl}e_{l}(x_{i+1}^{j})r_{l}^{j}(i+1)$ <br>\n",
    "where $x^{j}$, $f^{j}$, and $r^{j}$ are sequence, forward matrix, and backward matrix for squence index $j$ respectively.<br>\n",
    "Update emission matrix $E_{k}$ by summing over all positions ($i=1\\dots T$):<br>\n",
    "$E_{k}(\\sigma) = \\sum_{j}1/P(x^{j}) \\sum_{i|x_{i}^{j}=\\sigma}f_{k}^{j}(i)r_{k}^{j}(i)$ <br>\n",
    "where the inner sum is only over positions $i$ that have emission $\\sigma$. <br>\n",
    "Update initial state matrix:<br>\n",
    "$B_{k} = \\sum_{j}1/P(x^{j}) \\sum_{k}f_{k}^{j}(0)r_{k}^{j}(0)$\n",
    "\n",
    "Maximization:<br>\n",
    "Calculate new model parameters as we did with Markov Chains:<br>\n",
    "$a_{kl} = A_{kl}/\\sum_{l}A_{kl}$<br>\n",
    "$e_{k}(\\sigma) = E_{k}(\\sigma) / \\sum_{\\sigma}E_{k}(\\sigma)$<br>\n",
    "$b_{k} = B_{k} / \\sum_{k}{B_k}$\n",
    "\n",
    "<Br>Termination: <br>\n",
    "Stop at convergence as measured by log likelihood or is maximum number of iterations has been reached.\n",
    "\n",
    "Please add to the HMM class a new function baum_welch(self, sequences, pseudocount=1e-100) that takes as input a list of sequences to train the model. \n",
    "    \n",
    "I have added implementations of associated functions below, but you may use your own implementations from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class HMM(object):\n",
    "    \"\"\"Main class for HMM objects\n",
    "    \n",
    "    Class for holding HMM parameters and to allow for implementation of\n",
    "    functions associated with HMMs\n",
    "    \n",
    "    Private Attributes:\n",
    "        _alphabet (set): The alphabet of emissions\n",
    "        _hidden_states (set): Hidden states in the model\n",
    "        _transitions (dict(dict)): A dictionary of transition probabilities\n",
    "        _emissions (dict(dict)): A dictionary of emission probabilities\n",
    "        _initial (dict): A dictionary of initial state probabilities\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __all__ = ['viterbi', 'forward', 'backward', 'forward_backward', 'baum-welch']\n",
    "\n",
    "    def __init__(self, alphabet, hidden_states, A=None, E=None, B=None, seed=None):\n",
    "        self._alphabet = set(alphabet)\n",
    "        self._hidden_states = set(hidden_states)\n",
    "        self._transitions = A\n",
    "        self._emissions = E\n",
    "        self._initial = B\n",
    "        self._seed = seed\n",
    "        if(self._transitions == None):\n",
    "            self._initialize_random(self._alphabet, self._hidden_states, self._seed)\n",
    "            \n",
    "    def __str__(self):\n",
    "        out_text = [f'Alphabet: {self._alphabet}',\n",
    "                    f'Hidden States: {self._hidden_states}',\n",
    "                    f'Initial Probabilities: {json.dumps(self._initial, sort_keys = True, indent=4)}',\n",
    "                    f'Transition Probabilities: {json.dumps(self._transitions, sort_keys = True, indent=4)}',\n",
    "                    f'Emission Probabilities: {json.dumps(self._emissions, sort_keys = True, indent=4)}']\n",
    "        return '\\n'.join(out_text)\n",
    "    \n",
    "    @classmethod\n",
    "    def __dir__(cls):\n",
    "        return cls.__all__\n",
    "    \n",
    "    def _emit(self, cur_state, symbol):\n",
    "        return self._emissions[cur_state][symbol]\n",
    "    \n",
    "    def _transition(self, cur_state, next_state):\n",
    "        return self._transitions[cur_state][next_state]\n",
    "    \n",
    "    def _init(self, cur_state):\n",
    "        return self._initial[cur_state]\n",
    "\n",
    "    def _states(self):\n",
    "        for k in self._hidden_states:\n",
    "            yield k\n",
    "    \n",
    "    \n",
    "    def _get_alphabet(self):\n",
    "        for sigma in self._alphabet:\n",
    "            yield sigma\n",
    "            \n",
    "    def _initialize_random(self, alphabet, states, seed):\n",
    "        alphabet = list(set(alphabet))\n",
    "        alphabet.sort()\n",
    "        states = list(set(states))\n",
    "        states.sort()\n",
    "        self._alphabet = alphabet\n",
    "        self._hidden_states = states\n",
    "\n",
    "        #Initialize empty matrices A and E with pseudocounts\n",
    "        A = {}\n",
    "        E = {}\n",
    "        I = {}\n",
    "        np.random.seed(seed=seed)\n",
    "        I_rand = np.random.dirichlet(np.ones(len(self._hidden_states)))\n",
    "        for i, state in enumerate(self._states()):\n",
    "            E[state] = {}\n",
    "            A[state] = {}\n",
    "            I[state] = I_rand[i]\n",
    "            E_rand = np.random.dirichlet(np.ones(len(self._alphabet)))\n",
    "            A_rand = np.random.dirichlet(np.ones(len(self._hidden_states)))\n",
    "            for j, sigma in enumerate(self._get_alphabet()):\n",
    "                E[state][sigma] = E_rand[j]\n",
    "            for j, next_state in enumerate(self._states()):\n",
    "                A[state][next_state] = A_rand[j]\n",
    "                \n",
    "        self._transitions = A\n",
    "        self._emissions = E\n",
    "        self._initial = I\n",
    "        return\n",
    "    \n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\" The viterbi algorithm for decoding a string using a HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (list): optimal path through HMM given the model parameters\n",
    "                           using the Viterbi algorithm\n",
    "        \n",
    "        Pseudocode for Viterbi:\n",
    "            Initialization (𝑖=0): 𝑣𝑘(𝑖)=𝑒𝑘(𝜎)𝑏𝑘.\n",
    "            Recursion (𝑖=1…𝑇): 𝑣𝑙(𝑖)=𝑒𝑙(𝑥𝑖) max𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙); \n",
    "                                ptr𝑖(𝑙)= argmax𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙).\n",
    "            Termination: 𝑃(𝑥,𝜋∗)= max𝑘(𝑣𝑘(𝑙)𝑎𝑘0); \n",
    "                             𝜋∗𝑙= argmax𝑘(𝑣𝑘(𝑙)𝑎𝑘0).\n",
    "            Traceback: (𝑖=𝑇…1): 𝜋∗𝑖−1= ptr𝑖(𝜋∗𝑖).\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialization (𝑖=0): 𝑣𝑘(𝑖)=𝑒𝑘(𝜎)𝑏𝑘.\n",
    "        # Initialize trellis and traceback matrices\n",
    "        # trellis will hold the vi data as defined by Durbin et al.\n",
    "        # and trackback will hold back pointers (19)\n",
    "        trellis = {} # This only needs to keep the previous column probabilities\n",
    "        traceback = [] # This will need to hold all of the traceback data so will be an array of dicts()\n",
    "        for state in self._states():\n",
    "            trellis[state] = np.log10(self._init(state)) + np.log10(self._emit(state, sequence[0])) # b * e(0) for all k\n",
    "            \n",
    "        # Next we do the recursion step:\n",
    "        # Recursion (𝑖=1…𝑇): 𝑣𝑙(𝑖)=𝑒𝑙(𝑥𝑖) max𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙); \n",
    "        #                 ptr𝑖(𝑙)= argmax𝑘(𝑣𝑘(𝑖−1)𝑎𝑘𝑙).\n",
    "        for t in range(1, len(sequence)):  # For each position in the sequence\n",
    "            trellis_next = {}\n",
    "            traceback_next = {}\n",
    "\n",
    "            for next_state in self._states():    # Calculate maxk and argmaxk\n",
    "                k={}\n",
    "                for cur_state in self._states():\n",
    "                    k[cur_state] = trellis[cur_state] + np.log10(self._transition(cur_state, next_state)) # k(t-1) * a\n",
    "                argmaxk = max(k, key=k.get)\n",
    "                trellis_next[next_state] =  np.log10(self._emit(next_state, sequence[t])) + k[argmaxk] # k * e(t)\n",
    "                traceback_next[next_state] = argmaxk\n",
    "                \n",
    "            #Overwrite trellis \n",
    "            trellis = trellis_next\n",
    "            #Keep trackback pointer matrix\n",
    "            traceback.append(traceback_next)\n",
    "            \n",
    "        # Termination: 𝑃(𝑥,𝜋∗)= max𝑘(𝑣𝑘(𝑙)𝑎𝑘0); \n",
    "        #                  𝜋∗𝑙= argmax𝑘(𝑣𝑘(𝑙)𝑎𝑘0).\n",
    "        max_final_state = max(trellis, key=trellis.get)\n",
    "        max_final_prob = trellis[max_final_state]\n",
    "                \n",
    "        # Traceback: (𝑖=𝑇…1): 𝜋∗𝑖−1= ptr𝑖(𝜋∗𝑖).\n",
    "        result = [max_final_state]\n",
    "        for t in reversed(range(len(sequence)-1)):\n",
    "            result.append(traceback[t][max_final_state])\n",
    "            max_final_state = traceback[t][max_final_state]\n",
    "\n",
    "        return result[::-1]\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\" The forward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (float, list of dicts): P(x) and the f matrix as a list\n",
    "        \n",
    "        Pseudocode for Forward:\n",
    "            Initialization (𝑖=0): 𝑓𝑘(0)=𝑒𝑘(𝜎0)𝑏𝑘.\n",
    "            Recursion (𝑖=1…𝑇): 𝑓𝑙(𝑖)=𝑒𝑙(𝜎𝑖)∑𝑘(𝑓𝑘(𝑖−1)𝑎𝑘𝑙)\n",
    "            Termination: 𝑃(𝑥)=∑𝑘𝑓𝑘(𝑇)\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialization (𝑖=0): 𝑓𝑘(0)=𝑒𝑘(𝜎0)𝑏𝑘.\n",
    "        # Initialize f\n",
    "        f = [] # For this algorithm it is helpful to keep this entire matrix\n",
    "        f.append({})\n",
    "        for state in self._states():\n",
    "            f[-1][state] = self._init(state) * self._emit(state, sequence[0]) # b * e(0) for all k\n",
    "\n",
    "        # Next we do the recursion step:\n",
    "        # Recursion (𝑖=1…𝑇): 𝑓𝑙(𝑖)=𝑒𝑙(𝜎𝑖)∑𝑘(𝑓𝑘(𝑖−1)𝑎𝑘𝑙) \n",
    "        for i in range(1, len(sequence)):  # For each position in the sequence\n",
    "            f.append({})\n",
    "            for next_state in self._states(): # For each state\n",
    "                f[-1][next_state] = 0\n",
    "                for cur_state in self._states():\n",
    "                    f[-1][next_state] += f[i-1][cur_state] * self._transition(cur_state, next_state) # sum of f(i-1) * a\n",
    "                f[-1][next_state] =  self._emit(next_state, sequence[i]) * f[-1][next_state] # f * e(i)\n",
    "        \n",
    "        # Termination: 𝑃(𝑥)=∑𝑘𝑓𝑘(𝑇)\n",
    "        Px = 0\n",
    "        for state in self._states():\n",
    "            Px += f[-1][state]\n",
    "            \n",
    "        return Px, f\n",
    "\n",
    "    def backward(self, sequence):\n",
    "        \"\"\" The backward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (float, list of dicts): P(x) and the b matrix as a list\n",
    "        \n",
    "        Pseudocode for Backward:\n",
    "            Initialization (𝑖=T): 𝑟𝑘(𝑇)=1.\n",
    "            Recursion (𝑖=𝑇−1…1): 𝑟𝑘(𝑖)=∑𝑙𝑟𝑙(𝑖+1)𝑎𝑘𝑙𝑒𝑙(𝜎𝑖+1)\n",
    "            Termination: 𝑃(𝑥)=∑𝑙𝑟𝑘(1)𝑒𝑙(𝜎1)𝑏𝑙\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialization (𝑖=T): 𝑟𝑘(𝑇)=1.\n",
    "        # Initialize r\n",
    "        r = [] # For this algorithm it is helpful to keep this entire matrix\n",
    "        r.insert(0, {})\n",
    "        for state in self._states():\n",
    "            r[0][state] = 1 # 1 for all k\n",
    "\n",
    "        # Next we do the recursion step:\n",
    "        # Recursion (𝑖=T-1…1): 𝑟𝑘(𝑖)=∑𝑙𝑟𝑙(𝑖+1)𝑎𝑘𝑙𝑒𝑙(𝜎𝑖+1)\n",
    "        for i in range(len(sequence)-1, 0, -1):  # For each position in the sequence in reverse\n",
    "            r.insert(0, {}) # append a new item at the beginning\n",
    "            for prev_state in self._states(): # For each state\n",
    "                r[0][prev_state] = 0\n",
    "                for next_state in self._states():\n",
    "                    r[0][prev_state] += r[1][next_state] * self._transition(prev_state, next_state) * self._emit(next_state, sequence[i])\n",
    "\n",
    "        # Termination: 𝑃(𝑥)=∑𝑙𝑟𝑘(1)𝑒𝑙(𝜎1)𝑏𝑙\n",
    "        Px = 0\n",
    "        for state in self._states():\n",
    "            Px += r[0][state] * self._init(state) * self._emit(state, sequence[0])\n",
    "                        \n",
    "        return Px, r\n",
    "    \n",
    "    def forward_backward(self, sequence):\n",
    "        \"\"\" The forward-backward algorithm for calculating marginal posteriors given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            posterior (list of dicts): all posteriors as a list\n",
    "        \n",
    "        Pseudocode for Forward-Backward:\n",
    "            Calculate f[] as forward algorithm\n",
    "            Calculate r[] as backward algorithm\n",
    "            for all i in sequence\n",
    "                for all states\n",
    "                    posterior[i][state] = f[i][state] * r[i][state] / Px\n",
    "        \"\"\"        \n",
    "        #Calculate forward and backward matrices\n",
    "        f_Px, f_matrix = self.forward(sequence)\n",
    "        r_Px, r_matrix = self.backward(sequence)\n",
    "    \n",
    "        posterior = []\n",
    "        for i in range(0, len(sequence)):  # For each position in the sequence\n",
    "            posterior.append({})\n",
    "            for state in self._states(): # For each state\n",
    "                posterior[i][state] = f_matrix[i][state] * r_matrix[i][state] / f_Px\n",
    "                \n",
    "        return posterior\n",
    "    \n",
    "    def baum_welch(self, sequences, pseudocount=1e-100):\n",
    "        \"\"\" The baum-welch algorithm for unsupervised HMM parameter learning\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of sequences containing valid emissions from the HMM\n",
    "            pseudocount (float): small pseudocount value (default: 1e-100)\n",
    "\n",
    "        Returns:\n",
    "            None but updates the current HMM model parameters:\n",
    "             self._transitions, self._emissions, self._initial\n",
    "        \n",
    "        \"\"\"     \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code will initialize your HMM\n",
    "\n",
    "hidden_states = ('I', 'G') # CpG Island or Genome\n",
    "alphabet = ('A', 'C', 'G', 'T') # DNA Alphabet\n",
    "\n",
    "model = HMM(alphabet, hidden_states, seed=70)\n",
    "\n",
    "sequence = \"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\"\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example testing of the above implementation\n",
    "model.baum_welch([sequence])\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
